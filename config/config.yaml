data:
    train : /kaggle/working/rct/data/train.txt
    test : /kaggle/working/rct/data/test.txt
    dev: /kaggle/working/rct/data/dev.txt
    finetuned_model: /kaggle/working/rct/model/

pruning:
    model_name: /kaggle/working/rct/model/rct_bert_4/
    head_mask: {0: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10, 11], 1: [0, 1, 2, 3, 4, 5, 7, 8, 9, 10], 2: [2, 3, 4, 5, 6, 7, 8, 9, 10, 11], 3: [1, 3, 4, 6, 7, 9, 10, 11], 4: [0, 1, 2, 4, 5, 6, 7, 8, 10], 5: [0, 1, 2, 3, 5, 9, 11], 6: [0, 1, 2, 3, 4, 5, 6, 7, 9, 11], 7: [2, 4, 5, 6, 9, 10, 11], 8: [0, 1, 2, 3, 4, 5, 6, 8, 9, 10], 9: [0, 1, 2, 3, 4, 6, 7, 8, 9, 10, 11], 10: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], 11: [0, 2, 3, 6, 7, 8, 9, 11]}
    
    

finetunedmodel:
    model : /kaggle/working/rct/model/pytorch_model.bin
    config: /kaggle/working/rct/model/config.json
    vocab: /kaggle/working/rct/model/vocab.txt

hyperparams:
     learning_rate: 2e-5
     epsilon: 1e-8
     epochs: 3
     batch_size: 16
     

